{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Headlines-preprocessing\" data-toc-modified-id=\"Headlines-preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Headlines preprocessing</a></span></li><li><span><a href=\"#Texts-preprocessing\" data-toc-modified-id=\"Texts-preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Texts preprocessing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will focus on how the preprocessing will occur in the src/features/build_features.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dir_interim = '../data/interim'\n",
    "data_dir_processed = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folha_articles = pd.read_csv(os.path.join(data_dir_interim, 'news-of-the-site-folhauol/articles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = folha_articles['text'].tolist()\n",
    "headlines = folha_articles['title'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Headlines preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "headlines_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "headlines_tokenizer.fit_on_texts(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "encoded_headlines = headlines_tokenizer.texts_to_sequences(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the previous notebook (1.0-bgm-inital-data-exploration) we saw that a good length to pad our headlines would be 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "padded_headlines = tf.keras.preprocessing.sequence.pad_sequences(encoded_headlines, maxlen=16, \n",
    "                                                                 padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the articles, since we just want the context to create our headlines, we are going to drop stopwords and use lemmatization, we are also going to use the first paragraph of each article since we saw on the previous notebook that they contain enough information to generate the headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_paragraph(stn):\n",
    "    '''\n",
    "    Returns first paragraph of sentence\n",
    "    '''\n",
    "    if pd.isna(stn):\n",
    "        return ''\n",
    "    return stn.split('  ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(stn):\n",
    "    '''\n",
    "    Remove punctuation and lower case sentence\n",
    "    '''\n",
    "    if pd.isna(stn):\n",
    "        return ''\n",
    "    stn = stn.lower()\n",
    "    return ''.join([c for c in stn if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(texts):\n",
    "    '''\n",
    "    Remove portuguese stopwords from corpus\n",
    "    '''\n",
    "    new_texts = []\n",
    "    stop_words = nltk.snowball.stopwords.words('portuguese')\n",
    "    for stn in texts:\n",
    "        if pd.isna(stn):\n",
    "            new_texts.append('')\n",
    "        else:\n",
    "            new_texts.append(' '.join([word for word in stn.split() \n",
    "                                       if word.lower() not in stop_words]))\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_string(stn, lemmatizer):\n",
    "    '''\n",
    "    Lemmatize words in sentence\n",
    "    '''\n",
    "    new_stn = []\n",
    "    for token in lemmatizer(stn):\n",
    "        new_stn.append(token.lemma_)\n",
    "        \n",
    "    return ' '.join(new_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph_texts = [get_first_paragraph(stn) for stn in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = [preprocess_string(stn) for stn in first_paragraph_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = drop_stopwords(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 s, sys: 28.2 ms, total: 2.6 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(processed_texts)//1000):\n",
    "    stn = processed_texts[i]\n",
    "    lemmatized_texts.append(lemmatize_string(stn, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "texts_tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last notebook we can see a good length for the padded articles would be around 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = texts_tokenizer.texts_to_sequences(texts)\n",
    "padded_texts = tf.keras.preprocessing.sequence.pad_sequences(encoded_texts, maxlen=1000\n",
    "                                                             , truncating='post', padding='post')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
